package project1;
import java.io.IOException;
import java.util.StringTokenizer;
import java.io.BufferedReader;
import java.io.FileReader;
import java.util.HashMap;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;


public class Query5 extends Configured implements Tool {
	
	
	
	public static class JoinMapper
			extends Mapper<Object, Text, Text, IntWritable>{
		//Save records for Customers relation
		private HashMap<String, String> customerMap = new HashMap<String, String>();
		private Text ID = new Text();
		private IntWritable one = new IntWritable(1);
	
		//Read Customers.csv file into DistributedCache for map-side join
		@Override
		protected void setup(Context context
				) throws IOException, InterruptedException{
			BufferedReader bf = null;
			Path[] paths = DistributedCache.getLocalCacheFiles(context.getConfiguration());
			String line = "";
			
			for(Path path: paths){
				if(path.toString().contains("Customers")){
					bf = new BufferedReader(new FileReader(path.toString()));
					while((line=bf.readLine())!=null){
						// save to HashMap
						String[] str= line.split(",", 2);
						customerMap.put(str[0], str[1]);
					}
				}
			}
			
			bf.close();
		} // finish setup
		
		public void map(Object key, Text value, Context context
				)throws IOException, InterruptedException{
			StringTokenizer itr = new StringTokenizer(value.toString());
			while(itr.hasMoreTokens()){
				String[] recordT = itr.nextToken().toString().split(",");
				String[] recordC = customerMap.get(recordT[1]).split(",");
				// ID name as key
				ID.set(recordT[1]+" "+recordC[0]);
				//  #transaction as value
				context.write(ID, one);
			}
		}
	
	}// finish mapper
	
	public static class sumReducer 
			extends Reducer<Text, Text, Text, IntWritable>{
		
		public void reduce(Text key, Iterable<IntWritable> value, Context context
				) throws IOException, InterruptedException{
			int sum = 0;	//#transacion
			for(IntWritable val:value){
				sum += val.get();
			}
			IntWritable sum_ = new IntWritable(sum);
			context.write(key, sum_);
		}
	}// finish reducer(job 1) get id name #transaction
	

	
	@Override
	public int run(String[] arg0) throws Exception {
		Configuration conf = new Configuration();
		Job job = new Job(conf, "Query5");
		job.setJarByClass(Query5.class);
		job.setMapperClass(JoinMapper.class);
		job.setReducerClass(sumReducer.class);
		job.setNumReduceTasks(2);
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(Text.class);
		DistributedCache.addCacheFile(new Path(arg0[0]).toUri(), job
                .getConfiguration());
		FileInputFormat.addInputPath(job, new Path(arg0[1]));
		FileOutputFormat.setOutputPath(job, new Path(arg0[2]));
		
		return job.waitForCompletion(true) ? 0 : 1;
	}
	
	public static void main(String[] args) throws Exception{
		int res = ToolRunner.run(new Configuration(), new Query5(),
                args);
        System.exit(res);
	}
}